{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Practice with Sentiment Classifier \n",
    "## Cross Validation and Accuracy Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some formatting to make output bold\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some sentiment keywords\n",
    "# Positive and negative ones are treated the same in the model, but this ordering\n",
    "# effects how the heatmap displays them\n",
    "pos_keywords = ['great', 'love', 'awesome', 'amazing', 'fav', 'is',\n",
    "                'excellent', 'good', 'best', 'yummy', 'pleasure', 'wow', \n",
    "                'back', 'very', 'really', 'friendly', 'time', 'fantastic', \n",
    "                'fresh', 'over', 'recommend', 'definitely', 'quite', 'deal', 'happy',\n",
    "                'delicious', 'spicy', 'do', 'much', 'nice', 'wonderful', 'ambiance',\n",
    "                'enjoy', 'price']\n",
    "\n",
    "neg_keywords = ['not', 'angry', 'didn\\'t', 'disgusted', 'slow', 'nasty', \n",
    "                'dirty', 'avoid', 'disappointed', 'never', 'disappointing', \n",
    "                'poor', 'bad', 'wasted', 'wasn\\'t', 'rude', 'bland', 'mediocre']\n",
    "    \n",
    "keywords = pos_keywords + neg_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentiment(data_path, description):\n",
    "    review_raw = pd.read_csv(data_path, delimiter='\\t', header=None)\n",
    "    review_raw.columns = ['review', 'sentiment']\n",
    "    \n",
    "    # Exclamation marks more commonly found in positive reviews\n",
    "    review_raw['exclamation'] = review_raw.review.str.contains('!') \n",
    "\n",
    "    # Yelp model actually gets moderately worse removing these punctuations\n",
    "    punctuation = [',', '\\.', '!', '?']\n",
    "    for mark in punctuation:\n",
    "        review_raw['review'] = review_raw['review'].str.replace(mark, '')\n",
    "    \n",
    "    # Check for spaces before word to not get partial matches\n",
    "    for key in keywords:\n",
    "        review_raw[str(key)] = review_raw.review.str.contains(\n",
    "            ' ' + str(key), \n",
    "            case=False\n",
    "        )\n",
    "\n",
    "    #Thought maybe length of review would be related, but this didn't help\n",
    "    review_raw['long'] = review_raw.review.str.len()\n",
    "    review_raw['long'] = [0 if x < 80 else 1 for x in review_raw['long']]\n",
    "    \n",
    "    review_raw['upper'] = review_raw.review.apply(check_upper)\n",
    "\n",
    "    #fig, ax = plt.subplots(figsize=(15,15))\n",
    "    #sns.heatmap(review_raw.corr(), vmax=1, vmin=-1, cmap='coolwarm', ax=ax)\n",
    "    #plt.show()\n",
    "    \n",
    "    # specifying variables and outcomes for SKLearn\n",
    "    data = review_raw[pos_keywords + neg_keywords + ['exclamation'] + ['long'] + ['upper']]\n",
    "    target = review_raw['sentiment']\n",
    "    \n",
    "    printmd('**' + 'Dataset Information' + '**')\n",
    "    print('There are {} positive and {} negative reviews in this dataset\\n'\n",
    "          .format(target.sum(), len(target) - target.sum()))\n",
    "    \n",
    "    return review_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function looks to see if at least three words in the review are all uppercase\n",
    "# This tends to indicate either yelling or excitement\n",
    "def check_upper(review):\n",
    "    words = review.split(' ')\n",
    "    upper_bool = False\n",
    "    upper_count = 0\n",
    "    for word in words:\n",
    "        # I is normally capitalized, don't count it\n",
    "        if (word.isupper()) and (word != 'I'):\n",
    "            upper_count += 1\n",
    "    # Using less than 3 seems to make the model worse instead of better\n",
    "    if upper_count >= 3:\n",
    "        upper_bool = True\n",
    "    return upper_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models_and_print(description, data, target):   \n",
    "    \n",
    "    # Create model and fit all data\n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(data, target)\n",
    "    y_pred = bnb.predict(data)\n",
    "    \n",
    "    # Display our results, not split\n",
    "    print('Bernoulli Naive Bayes.')\n",
    "    print('Number of mislabeled points out of a total {} points : {}\\n'.format(\n",
    "        data.shape[0], \n",
    "        (target != y_pred).sum()\n",
    "    ))\n",
    "    print('Score for no holdouts: '+ str(bnb.score(data, target)))\n",
    "    print('Confusion Matrix no holdouts:' + '\\n' + str(confusion_matrix(target, y_pred)))\n",
    "    \n",
    "    # Split into test and training datasets and train again\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    "    bnb.fit(X_train, y_train)\n",
    "    y_pred_test = bnb.predict(X_test)\n",
    "    \n",
    "    print('\\nScore with 20% Holdout: ' + str(bnb.score(X_test, y_test)))\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n",
    "    \n",
    "    print('\\nCross Validation Scores:')\n",
    "    print(cross_val_score(bnb, data, target, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write up\n",
    "\n",
    "#### Features selected:\n",
    "I used a list of key words, exclamation marks, length of the review, and whether or not there were three or more all caps words in the review as my features.  I added key words to the list until adding additional key words made no more improvement to the model. I checked this by iterating through all the words (this would be trickier to do in a larger data set) used in at least 5 different reviews.  Something that was interesting to me was that many of the most frequent words are very neutral words and not used overly frequently.\n",
    "\n",
    "Removing punctuation made the model perform slightly worse. \n",
    "\n",
    "Many of the words that helped the yelp reviews sentiment model were not found in the Amazon reviews.  However, improving the Yelp model did also improve the Amazon model.  \n",
    "\n",
    "It is likely that this model is somewhat overfit to the dataset since I did not split the data set into training and test datasets.  Future projects should be split into these subsets before fitting the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Dataset Information**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 500 positive and 500 negative reviews in this dataset\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Model 1 - Original Model**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes.\n",
      "Number of mislabeled points out of a total 1000 points : 179\n",
      "\n",
      "Score for no holdouts: 0.821\n",
      "Confusion Matrix no holdouts:\n",
      "[[447  53]\n",
      " [126 374]]\n",
      "\n",
      "Score with 20% Holdout: 0.775\n",
      "Confusion Matrix\n",
      "[[86 20]\n",
      " [25 69]]\n",
      "\n",
      "Cross Validation Score:\n",
      "[0.78 0.87 0.78 0.84 0.8  0.73 0.86 0.81 0.68 0.86]\n"
     ]
    }
   ],
   "source": [
    "data_path = 'sentiment labelled sentences/yelp_labelled.txt'\n",
    "\n",
    "reviews = check_sentiment(data_path, 'Yelp')\n",
    "\n",
    "printmd('**' + 'Model 1 - Original Model' + '**')\n",
    "data1 = reviews[pos_keywords + neg_keywords + ['exclamation'] + ['long'] + ['upper']]\n",
    "target = reviews['sentiment']\n",
    "\n",
    "run_models_and_print('Yelp', data1, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model leans towards predicting negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Model 2 - Exclude Exclamations**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes.\n",
      "Number of mislabeled points out of a total 1000 points : 200\n",
      "\n",
      "Score for no holdouts: 0.8\n",
      "Confusion Matrix no holdouts:\n",
      "[[463  37]\n",
      " [163 337]]\n",
      "\n",
      "Score with 20% Holdout: 0.815\n",
      "Confusion Matrix\n",
      "[[93  5]\n",
      " [32 70]]\n",
      "\n",
      "Cross Validation Score:\n",
      "[0.73 0.86 0.8  0.79 0.67 0.74 0.8  0.79 0.7  0.86]\n"
     ]
    }
   ],
   "source": [
    "printmd('**' + 'Model 2 - Exclude Exclamations' + '**')\n",
    "\n",
    "data2 = reviews[pos_keywords + neg_keywords + ['long'] + ['upper']]\n",
    "\n",
    "run_models_and_print('Yelp', data2, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is leaning towards predicting negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Model 3 - Exclude Uppercase**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes.\n",
      "Number of mislabeled points out of a total 1000 points : 182\n",
      "\n",
      "Score for no holdouts: 0.818\n",
      "Confusion Matrix no holdouts:\n",
      "[[445  55]\n",
      " [127 373]]\n",
      "\n",
      "Score with 20% Holdout: 0.795\n",
      "Confusion Matrix\n",
      "[[88  6]\n",
      " [35 71]]\n",
      "\n",
      "Cross Validation Score:\n",
      "[0.77 0.87 0.78 0.84 0.79 0.73 0.86 0.81 0.69 0.85]\n"
     ]
    }
   ],
   "source": [
    "printmd('**' + 'Model 3 - Exclude Uppercase' + '**')\n",
    "\n",
    "data3 = reviews[pos_keywords + neg_keywords + ['exclamation'] + ['long']]\n",
    "\n",
    "run_models_and_print('Yelp', data3, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Model 4 - Exclude neg keywords**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes.\n",
      "Number of mislabeled points out of a total 1000 points : 255\n",
      "\n",
      "Score for no holdouts: 0.745\n",
      "Confusion Matrix no holdouts:\n",
      "[[412  88]\n",
      " [167 333]]\n",
      "\n",
      "Score with 20% Holdout: 0.67\n",
      "Confusion Matrix\n",
      "[[70 33]\n",
      " [33 64]]\n",
      "\n",
      "Cross Validation Score:\n",
      "[0.7  0.77 0.71 0.76 0.71 0.61 0.73 0.75 0.67 0.75]\n"
     ]
    }
   ],
   "source": [
    "printmd('**' + 'Model 4 - Exclude neg keywords' + '**')\n",
    "\n",
    "data4 = reviews[pos_keywords + ['exclamation'] + ['long'] + ['upper']]\n",
    "\n",
    "run_models_and_print('Yelp', data4, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model over-predicted negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Model 5 - No positive keywords**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes.\n",
      "Number of mislabeled points out of a total 1000 points : 330\n",
      "\n",
      "Score for no holdouts: 0.67\n",
      "Confusion Matrix no holdouts:\n",
      "[[272 228]\n",
      " [102 398]]\n",
      "\n",
      "Score with 20% Holdout: 0.63\n",
      "Confusion Matrix\n",
      "[[61 47]\n",
      " [27 65]]\n",
      "\n",
      "Cross Validation Score:\n",
      "[0.7  0.64 0.72 0.6  0.6  0.65 0.71 0.66 0.59 0.66]\n"
     ]
    }
   ],
   "source": [
    "printmd('**' + 'Model 5 - No positive keywords' + '**')\n",
    "\n",
    "data5 = reviews[neg_keywords + ['exclamation'] + ['long'] + ['upper']]\n",
    "\n",
    "run_models_and_print('Yelp', data5, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model without positive keywords predicted more positive reviews.  This is at the expense of correctly predicting negative reviews. The cross validation on this varies a fair amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
